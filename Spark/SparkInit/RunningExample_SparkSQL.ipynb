{"cells":[{"cell_type":"markdown","source":["## Spark SQL - Running Example\n\n**Report By** : OUMOUSS EL MEHDI (M2 Data & Knowledge)"],"metadata":{}},{"cell_type":"markdown","source":["**Paper - Spark SQL: Relational Data Processing in Spark**\n<p> https://pdfs.semanticscholar.org/f845/6b259bbf137ba89db548d77ab6643a2e40b2.pdf"],"metadata":{}},{"cell_type":"markdown","source":["![About Spark](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/book_intro/spark_about.png)"],"metadata":{}},{"cell_type":"markdown","source":["**The SparkSession, which is going to be our access point to the Spark Framework:**"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **DataFrames API**"],"metadata":{}},{"cell_type":"markdown","source":["**1. Using Semi-structed DataSet (Json) : **"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.json('/FileStore/tables/rj2rp8ke1488295662051/moviepeople_1000-1c188.json')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df.show(5)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Notice that the above cell takes 0.07 seconds to infer the schema by sampling the file and reading through it.\n\nInferring the schema works for ad hoc analysis against smaller datasets. But when working on multi-TB+ data, it's better to provide an **explicit pre-defined schema manually**, so there's no inferring cost."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Count how many rows total there are in DataFrame (and see how long it takes to do a full scan from remote disk):"],"metadata":{}},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["**Q. show the first 5 people in the dataSet**"],"metadata":{}},{"cell_type":"code","source":["df.select('person-name').take(5)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["**Q. The person named Anabela Teixeira**"],"metadata":{}},{"cell_type":"code","source":["df.filter(df['person-name']=='Teixeira, Anabela').show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["**Q. The birthplace of Steven Spielberg**"],"metadata":{}},{"cell_type":"code","source":["df.filter(df['person-name']=='Spielberg, Steven').select('info.birthdate', 'info.birthnotes').show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["**2. Using CSV File : Fire Safety Complaints**\n\nInformation on Complaints received by the Fire Department (from the public) for a particular location. Key fields include Complaint Number, Complaint Type, Address, Disposition\n\nDataset : https://data.sfgov.org/Housing-and-Buildings/Fire-Safety-Complaints/2wsq-7wmv \n\nAPI Doc: https://dev.socrata.com/foundry/data.sfgov.org/v3w9-dyka"],"metadata":{}},{"cell_type":"code","source":["df_csv = spark.read.load('/FileStore/tables/9rlemeo91488310038961/Fire_Safety_Complaints.csv',format='com.databricks.spark.csv', header='true', inferSchema='true')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df_csv.printSchema()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df_csv.columns"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["df_csv = df_csv\\\n.withColumnRenamed(\"Neighborhood  District\", \"NeighborhoodDistrict\")\\\n.withColumnRenamed(\"Complaint Item Type Description\", \"ComplaintItemTypeDescription\")\\\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["**Q. How many complaints of each complaints type were there?**"],"metadata":{}},{"cell_type":"code","source":["display(df_csv.select('ComplaintItemTypeDescription').groupBy('ComplaintItemTypeDescription').count().orderBy(\"count\", ascending=False))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["The SF Fire department receive complaints from alarm systems more than any other type. \n\nNote that the above command took about 2.15 seconds to execute. \nIn an upcoming section, we'll cache the data into memory for up to 100x speed increases."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom_pattern = 'dd/MM/yyyy'\nto_pattern = 'yyyy-MM-dd'\n\ndf_csv = df_csv \\\n  .withColumn('ReceivedDate', unix_timestamp(df_csv['Received Date'], from_pattern).cast(\"timestamp\")) \\\n  .drop('Received Date') \\\n  .withColumn('EntryDate', unix_timestamp(df_csv['Entry Date'], from_pattern).cast(\"timestamp\")) \\\n  .drop('Entry Date')"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(df_csv)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df_csv.printSchema()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["**Q. How many years of Fire Complaints is in the data file?**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df_csv.select(year('ReceivedDate')).distinct().orderBy('year(ReceivedDate)').show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["**Q-4) How many complaints were received before Feb. 27th 2017 ?**"],"metadata":{}},{"cell_type":"markdown","source":["Note above that Feb 27th, 2017 is the 58th day of the year."],"metadata":{}},{"cell_type":"code","source":["display(df_csv.filter(year('ReceivedDate') == '2017').filter(dayofyear('ReceivedDate') <= 58).groupBy(dayofyear('ReceivedDate')).count().orderBy('dayofyear(ReceivedDate)'))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) ** Memory and Caching **"],"metadata":{}},{"cell_type":"markdown","source":["The DataFrame is currently comprised of 2 partitions:"],"metadata":{}},{"cell_type":"code","source":["df_temp = df_csv\ndf_temp = df_temp.select('*', year('ReceivedDate').alias('ReceivedYear'))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["df_temp.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["df_temp.createOrReplaceTempView(\"df_VIEW\");\ndf_temp.repartition(6).createOrReplaceTempView(\"df_VIEW\");\nspark.catalog.cacheTable(\"df_VIEW\")"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["spark.table(\"df_VIEW\").count()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["fireComplaintsDF = spark.table(\"df_VIEW\")\nfireComplaintsDF.count()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Note that the full scan + count took less time (0.12 s compared to 0.78 seconds)"],"metadata":{}},{"cell_type":"code","source":["spark.catalog.isCached(\"df_VIEW\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **SQL Queries**"],"metadata":{}},{"cell_type":"code","source":["%sql SELECT count(*) FROM df_VIEW;"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["%sql SELECT NeighborhoodDistrict, ReceivedYear FROM df_VIEW"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["**Q. Which neighborhood in SF generated the most calls this year?**"],"metadata":{}},{"cell_type":"code","source":["%sql SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count FROM df_VIEW WHERE ReceivedYear == '2017' GROUP BY NeighborhoodDistrict ORDER BY Neighborhood_Count DESC LIMIT 15"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["SQL also has some handy commands like `DESC` (describe) to see the schema + data types for the table:"],"metadata":{}},{"cell_type":"code","source":["%sql DESC df_VIEW;"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) ** Spark Internals and SQL UI**"],"metadata":{}},{"cell_type":"markdown","source":["![Catalyst](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/catalyst.png)"],"metadata":{}},{"cell_type":"code","source":["# Note that a SQL Query just returns back a DataFrame\nspark.sql(\"SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count FROM df_VIEW WHERE ReceivedYear == '2017' GROUP BY NeighborhoodDistrict ORDER BY Neighborhood_Count DESC LIMIT 15\")"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["The `explain()` method can be called on a DataFrame to understand its logical + physical plans:"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count FROM df_VIEW WHERE ReceivedYear == '2017' GROUP BY NeighborhoodDistrict ORDER BY Neighborhood_Count DESC LIMIT 15\").explain(True)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["We can view the visual representation of the SQL Query plan from the Spark UI."],"metadata":{}},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) ** Spark MLlib **"],"metadata":{}},{"cell_type":"markdown","source":["####1. Loading data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import DecisionTree\nfrom pyspark.mllib.util import MLUtils\n\n# Load data file \ndata = MLUtils.loadLibSVMFile(sc, '/FileStore/tables/23phx7tu1475934034017/sample_libsvm_data.txt')\n        # Split the data into training and test sets (40% held out for testing)\n(trainingData, testData) = data.randomSplit([0.6, 0.4])"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["####2. Training"],"metadata":{}},{"cell_type":"code","source":["# Train a DecisionTree model.\n#  Empty categoricalFeaturesInfo indicates all features are continuous.\nmodel = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n                                     impurity='gini', maxDepth=5, maxBins=32)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["####3. Prediction & Evaluation"],"metadata":{}},{"cell_type":"code","source":["# Evaluate model on test instances and compute test error\npredictions = model.predict(testData.map(lambda x: x.features))\nlabelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\ntestErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\nprint('Test Error = ' + str(testErr))\nprint('Learned classification tree model:')\nprint(model.toDebugString())"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["**References : **\n\n1) Spark 2.0 preview docs: https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/\n\n2) DataFrame user documentation: https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/sql-programming-guide.html\n\n3) PySpark API 2.0 docs: https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/index.html\n\n4) Spark SQL : http://spark.apache.org/docs/latest/sql-programming-guide.html - http://spark.apache.org/sql/"],"metadata":{}}],"metadata":{"name":"RunningExample_SparkSQL","notebookId":4213424352664748},"nbformat":4,"nbformat_minor":0}
